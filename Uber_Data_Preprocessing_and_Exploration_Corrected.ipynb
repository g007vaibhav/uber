{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load and exploration of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\vaibh\\\\Downloads\\\\uber.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "stats = df.describe()\n",
    "\n",
    "# Passenger count distribution\n",
    "passenger_counts = df['passenger_count'].value_counts()\n",
    "\n",
    "print(stats)\n",
    "print(passenger_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fare_summary = df['fare_amount'].describe()\n",
    "\n",
    "# Visualize the distribution of fare_amount\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['fare_amount'], bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Fare Amount ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Fare Amount')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Output summary and visualization\n",
    "fare_summary, plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and testing of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =df.drop(columns=['fare_amount'])\n",
    "y = df['fare_amount']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['day_of_week', 'time_of_day', 'distance_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows = df[df[['dropoff_longitude', 'dropoff_latitude']].isnull().any(axis=1)]\n",
    "\n",
    "# Remove rows with missing values\n",
    "uber_data_clean = df.dropna()\n",
    "\n",
    "# Check if any missing values remain\n",
    "remaining_missing = df.isnull().sum()\n",
    "\n",
    "# Output the count of rows removed and the summary of missing values\n",
    "num_removed = len(missing_rows)\n",
    "remaining_missing, num_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the number of rows in the training and testing sets\n",
    "train_size = len(train_data)\n",
    "test_size = len(test_data)\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select numerical columns to standardize\n",
    "numerical_columns = [ 'fare_amount', 'passenger_count']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply standardization on training data\n",
    "train_data_scaled = train_data.copy()\n",
    "train_data_scaled[numerical_columns] = scaler.fit_transform(train_data[numerical_columns])\n",
    "\n",
    "# Apply the scaler fitted to training data on the testing data\n",
    "test_data_scaled = test_data.copy()\n",
    "test_data_scaled[numerical_columns] = scaler.transform(test_data[numerical_columns])\n",
    "\n",
    "# Verify the transformation\n",
    "train_data_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = ['fare_amount', 'pickup_longitude', 'pickup_latitude', \n",
    "                    'dropoff_longitude', 'dropoff_latitude', 'passenger_count']\n",
    "\n",
    "# Calculating the correlation matrix\n",
    "correlation_matrix = df[relevant_columns].corr()\n",
    "\n",
    "# Extracting the correlation with fare_amount\n",
    "fare_correlation = correlation_matrix['fare_amount'].sort_values(ascending=False)\n",
    "fare_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Training the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Making predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculating evaluation metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    metrics = (mse, mae, r2)\n",
    "except Exception as e:\n",
    "    metrics = str(e)\n",
    "    metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = df.sample(n=10000, random_state=1)\n",
    "\n",
    "# Convert 'pickup_datetime' from string to datetime\n",
    "sample_data['pickup_datetime'] = pd.to_datetime(sample_data['pickup_datetime'])\n",
    "\n",
    "# Extracting hour, day of the week, and month from 'pickup_datetime'\n",
    "sample_data['hour'] = sample_data['pickup_datetime'].dt.hour\n",
    "sample_data['day_of_week'] = sample_data['pickup_datetime'].dt.dayofweek\n",
    "sample_data['month'] = sample_data['pickup_datetime'].dt.month\n",
    "\n",
    "# Checking for missing values and out of bounds coordinates again with the sample\n",
    "missing_values_sample = sample_data.isnull().sum()\n",
    "out_of_bounds_sample = sample_data[\n",
    "    (sample_data['pickup_latitude'] < 40.5) | (sample_data['pickup_latitude'] > 41) |\n",
    "    (sample_data['pickup_longitude'] < -74.25) | (sample_data['pickup_longitude'] > -73.75) |\n",
    "    (sample_data['dropoff_latitude'] < 40.5) | (sample_data['dropoff_latitude'] > 41) |\n",
    "    (sample_data['dropoff_longitude'] < -74.25) | (sample_data['dropoff_longitude'] > -73.75)\n",
    "]\n",
    "\n",
    "missing_values_sample, out_of_bounds_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = sample_data[\n",
    "    (sample_data['pickup_latitude'] >= 40.5) & (sample_data['pickup_latitude'] <= 41) &\n",
    "    (sample_data['pickup_longitude'] >= -74.25) & (sample_data['pickup_longitude'] <= -73.75) &\n",
    "    (sample_data['dropoff_latitude'] >= 40.5) & (sample_data['dropoff_latitude'] <= 41) &\n",
    "    (sample_data['dropoff_longitude'] >= -74.25) & (sample_data['dropoff_longitude'] <= -73.75)\n",
    "]\n",
    "\n",
    "# Display the shape of the cleaned data\n",
    "cleaned_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Preparing the feature matrix and target vector\n",
    "X = cleaned_data[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'hour', 'day_of_week', 'month']]\n",
    "y = cleaned_data['fare_amount']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Initializing and training the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Prepare your feature matrix and target vector\n",
    "X = cleaned_data[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'hour', 'day_of_week', 'month']]\n",
    "y = cleaned_data['fare_amount']\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE: {mse}, MAE: {mae}, R^2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setup the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees\n",
    "    'max_depth': [10, 20, 30],        # Maximum depth of trees\n",
    "    'min_samples_split': [2, 5, 10]   # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error', verbose=2)\n",
    "\n",
    "# Fit grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predict and evaluate using the best estimator\n",
    "y_pred = best_rf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"MSE: {mse}, MAE: {mae}, R^2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
